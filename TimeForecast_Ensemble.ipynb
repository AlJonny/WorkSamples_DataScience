{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAxY6325X_YP"
   },
   "source": [
    "# Forecasting county-based microbusiness density levels across the 50 US states and District of Columbia\n",
    "\n",
    "#### D. Duvall, A. Johnson, K. Paeng, C. Silva, & Y. Zhang\n",
    "#### IMT 574--University of Washington\n",
    "#### Mike Stepanovic\n",
    "#### 11 March, 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc5CAWQ3G5zI"
   },
   "source": [
    "### Linear Model, Decision Tree, and Combined Model Approach from Time Series Analysis\n",
    "\n",
    "> ##### Code copied and adapted from Chirag Choudhary's Kaggle codebook (Feb 15th, 2023)\n",
    "> #### https://www.kaggle.com/code/ch124uec/time-series-hybrid-modeling\n",
    "\n",
    "\n",
    "In this investigation, we will train a series of Linear Regression models across unique counties in the dataset to eventually integrate with a larger forecasting algorithm based on decision tree regression method.\n",
    "\n",
    "The inspiration behind the logic for this algorithm was derived from a Kaggle online course on time series-based forecasting (see Holbrook in references). In Holbrook's online course, we see method that builds forecasting algorithm via combination of linear regression and decision tree modeling.\n",
    "\n",
    "With Linear Regression, we address seasonality to de-trend our microbusiness_density data across each unique county (cfips) series. We see the benefit and applicability of this with our first two linear model renditions.\n",
    "\n",
    "With decision trees, we address autocorrelation cycles to fit given data and then forecast with its integration into a final combined model informed by the respective linear regression and decision tree models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2stlUdvIjTNa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import read_csv, set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import csv\n",
    "#!pip install -U kaleido\n",
    "import kaleido\n",
    "import plotly.graph_objs as go\n",
    "from kaleido.scopes.plotly import PlotlyScope\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('path/to/image.png')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve, mean_squared_error, mean_absolute_error, r2_score\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree, export_graphviz\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from datetime import datetime\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#!pip install forecast\n",
    "#from forecast import smape\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import IFrame\n",
    "#IFrame(\"https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting\", width=700, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_path = \"/Users/alxbj/Downloads/census_starter.csv\"\n",
    "train_path = \"/Users/alxbj/Downloads/train.csv\"\n",
    "test_path = \"/Users/alxbj/Downloads/test.csv\"\n",
    "sample_path = \"/Users/alxbj/Downloads/sample_submission.csv\"\n",
    "revealed_path = \"/Users/alxbj/Downloads/revealed_test.csv\"\n",
    "\n",
    "census_df = pd.read_csv(census_path)\n",
    "census_df = pd.DataFrame(census_df)\n",
    "\n",
    "sample_df = pd.read_csv(sample_path)\n",
    "sample_df = pd.DataFrame(sample_df)\n",
    "\n",
    "# filling the NaNs\n",
    "numeric_cols = census_df.select_dtypes(include = ['float64', 'int64']).columns\n",
    "imputer = SimpleImputer(strategy = 'mean').fit(census_df[numeric_cols])\n",
    "census_df[numeric_cols] = imputer.transform(census_df[numeric_cols])\n",
    "\n",
    "test_df = pd.read_csv(test_path)\n",
    "test_df = pd.DataFrame(test_df)\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "train_df = pd.DataFrame(train_df)\n",
    "\n",
    "revealed_df = pd.read_csv(revealed_path)\n",
    "revealed_df = pd.DataFrame(revealed_df)\n",
    "revealed_df\n",
    "\n",
    "train_df = pd.concat([train_df, revealed_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoBdnLkxG3rL"
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "#census_df = pd.read_csv('census_starter.csv', sep=\",\",header=0)\n",
    "#census_df = pd.DataFrame(census_df)\n",
    "#census_df = census_df.apply(pd.to_numeric, errors='coerce').fillna(census_df)\n",
    "\n",
    "#sample_df = pd.read_csv('sample_submission.csv', sep=\",\",header=0)\n",
    "#sample_df = pd.DataFrame(sample_df)\n",
    "\n",
    "# filling the NaNs\n",
    "#numeric_cols = census_df.select_dtypes(include = ['float64', 'int64']).columns\n",
    "#imputer = SimpleImputer(strategy = 'mean').fit(census_df[numeric_cols])\n",
    "#census_df[numeric_cols] = imputer.transform(census_df[numeric_cols])\n",
    "\n",
    "# Read in the data (again)\n",
    "#test_df = pd.read_csv('test.csv', sep=\",\",header=0)\n",
    "#test_df = pd.DataFrame(test_df)\n",
    "\n",
    "#train_df = pd.read_csv('train.csv', sep=\",\",header=0)\n",
    "#train_df = pd.DataFrame(train_df)\n",
    "\n",
    "#revealed_df = pd.read_csv('revealed_test.csv', sep=\",\",header=0)\n",
    "#revealed_df = pd.DataFrame(revealed_df)\n",
    "\n",
    "#train_df = pd.concat([train_df, revealed_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DENi9la5G3uB"
   },
   "outputs": [],
   "source": [
    "test_df = test_df.merge(train_df[['cfips', 'county', 'state']].drop_duplicates(), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "iM6sOMZugJW3",
    "outputId": "2734b720-2462-447e-cd75-5e3ea1fa3789"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "g_h8QhXiltuq",
    "outputId": "68ed26f9-fd5a-4b97-aa28-ff6d23bd5d53"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-G9--DEctAs"
   },
   "source": [
    "Target variable will be 'microbusiness_density'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIZIfyU4G3xF"
   },
   "outputs": [],
   "source": [
    "target = 'microbusiness_density'\n",
    "#target = train_df[['microbusiness_density']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2WlmMEtG30T"
   },
   "outputs": [],
   "source": [
    "# merge train and test sets for main df\n",
    "df = pd.concat([train_df, test_df])\n",
    "\n",
    "df['row_id'] = df['row_id'].str.split('_', expand=True)[0]\n",
    "df['first_day_of_month'] = pd.to_datetime(df['first_day_of_month'])\n",
    "df['state_fips'] = df['cfips'] // 1000\n",
    "df['county_fips'] = df['cfips'] % 1000\n",
    "\n",
    "train_df = df.iloc[:len(train_df)]\n",
    "test_df = df.iloc[len(train_df):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbpamsB8hCrl"
   },
   "source": [
    "### A baseline linear regression model\n",
    "\n",
    "According to Holbrook's \"Time series\" seminar (see Kaggle.com), linear regression models can be adapted to time series data in flexible manners such\n",
    "that we may model higher order polynomial regressors to fit data.\n",
    "\n",
    "For the linear baseline model below, we begin by plotting microbusiness_density\n",
    "growth for one county over the entire time series (July, 2019-January, 2023).\n",
    "\n",
    "For each cfips value across the dataset, we must tailor our regression algorithm to fit a unique county's linear regression curve. \n",
    "\n",
    "To customize each regression and define its approach to capturing the moving average of our microbusiness_density values across the set, we call a new 'y_pred' to difference each county-based series.\n",
    "\n",
    "With this first linear regression, we will simply look at regressions across order 1-4 to identify the model with best residual minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KACX6fvYJrhE",
    "outputId": "47b72627-f6e5-4c61-a809-d67c04023cf5"
   },
   "outputs": [],
   "source": [
    "# create rule to randomly draw a bucket featuring a singular cfip (county) \n",
    "# series of  microbusiness_density measures across the 4yr data\n",
    "cfips = np.random.choice(train_df['cfips'])\n",
    "cfips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mC-dWE2G33J",
    "outputId": "b4e892ce-a5a1-4240-9dbe-cd740eb053e4"
   },
   "outputs": [],
   "source": [
    "# temporary subset_df contains 41 values of microbusiness data\n",
    "subset_df = train_df.loc[train_df['cfips']==cfips].set_index(['first_day_of_month'])\n",
    "subset_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df\n",
    "#note subset_df shows the microbiz density metrics for cfips drawn using random choice--you get new county each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "LTm4Q8ocG36a",
    "outputId": "760cd81e-43e9-48dc-af38-d90e4dce1e81"
   },
   "outputs": [],
   "source": [
    "fig = px.line(subset_df, y=target, markers=True, \n",
    "              title=f\"Plot of {target} for county = cfips {cfips}\")\n",
    "fig.show()\n",
    "#fig.write_image('my_plotly_chart.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![my_plotly_chart.png](attachment:e70871f2-04bb-4421-a1ee-6d82260be57d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UiEjua8q-rbS",
    "outputId": "ddaa33bb-935e-4967-c04d-f2fc0de0fba0"
   },
   "outputs": [],
   "source": [
    "lowest_resid = float('inf')\n",
    "lowest_mse = float('inf')\n",
    "lowest_smape = float('inf')\n",
    "best_model_order = None\n",
    "\n",
    "for n in range(3):\n",
    "    dp = DeterministicProcess(\n",
    "        index=subset_df.index,\n",
    "        constant=True,\n",
    "        order=n+1,\n",
    "        drop=True,\n",
    "    )\n",
    "\n",
    "    X = dp.in_sample()\n",
    "    y = subset_df[target]\n",
    "\n",
    "    trend_model = LinearRegression()\n",
    "    trend_model.fit(X, y)\n",
    "\n",
    "    y_pred = pd.Series(trend_model.predict(X), index=X.index)\n",
    "\n",
    "    fig = px.line(subset_df, y=target, markers=True, \n",
    "                  title=f\"Plot of {target} for county = cfips {cfips} : Linear Regression Model (Order {n+1})\")\n",
    "    \n",
    "    fig.add_scatter(x=y_pred.index, y=y_pred, line=dict(dash='dash'), name=f\"Order {n+1} Trend\")\n",
    "    \n",
    "    fig.update_layout(showlegend=True)\n",
    "    fig.show()\n",
    "    fig.write_image('my_plotly_chart_2.png')\n",
    "    \n",
    "    # Compute cumulative residual valuation, mean squared error, and SMAPE\n",
    "    resid_val = ((y - y_pred)**2).sum()\n",
    "    mse_val = mean_squared_error(y, y_pred)\n",
    "    smape_val = np.mean(np.abs((y - y_pred) / (y + y_pred) / 2)) * 100\n",
    "    \n",
    "    # Check if current model has lower residual valuation, mean squared error, or SMAPE\n",
    "    if resid_val < lowest_resid:\n",
    "        lowest_resid = resid_val\n",
    "        best_model_order = n+1\n",
    "        \n",
    "    if mse_val < lowest_mse:\n",
    "        lowest_mse = mse_val\n",
    "        \n",
    "    if smape_val < lowest_smape:\n",
    "        lowest_smape = smape_val\n",
    "        \n",
    "# Print out results for model with lowest cumulative residual valuation, mean squared error, and SMAPE\n",
    "print(f\"The best model is the one with order {best_model_order}, which has a cumulative residual valuation of {lowest_resid:.2f}, a mean squared error of {lowest_mse:.2f}, and a SMAPE of {lowest_smape:.2f}%.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![my_plotly_chart_2.png](attachment:8951506a-465c-44a3-85ba-ee3588a85534.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maiHJyrrgtg4"
   },
   "source": [
    "#### Noting best model iterant accuracy in order 3 polynomial fit\n",
    "\n",
    "In terms of mean squared error and cumulative residuals, we see that the linear model can be improved perhaps with more appropriate time-based treatment of our series; this will be addressed with a seasonality-informed linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5nwrEw-gbw2"
   },
   "source": [
    "### Linear Regression Model--Seasonality Rendition\n",
    "\n",
    "We follow Holbrook's method from the Kaggle time series course to difference and then use a seasonality hyperparameter in the 'DeterministicProcess' function to better approximate our target value (microbusiness_density)--effectively, this is \"de-trending\" the series.\n",
    "\n",
    "For the last baseline linear regression where we indexed solely with respect to the subset_df's 'first_day_of_the_month' feature, we now index the new model with respect to the entire series of microbusiness_density valuations.\n",
    "\n",
    "This widescale view will help to identify seasonal-based trends with the addition of 'seasonal = True' hyperparameter. \n",
    "\n",
    "How much more accurate can linear approximation be with these tunings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__fCVVgEG4KF"
   },
   "outputs": [],
   "source": [
    "# Differencing for stationising time series\n",
    "y = y - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "J2Y-3fAUG4NC",
    "outputId": "7955b2fe-3fe2-45b1-ac11-b36c5705bf93"
   },
   "outputs": [],
   "source": [
    "fig = px.line(y, markers=True, title=f\"Plot of {target} for county = cfips {cfips} De-trended and de-seasoned time series\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image('my_plotly_chart_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![my_plotly_chart_3.png](attachment:25990d3d-7480-401f-9b3e-d91278985591.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qlrc4LlPBvaw",
    "outputId": "42074816-33c8-4bb7-a8fb-ecdc2a38eaa6"
   },
   "outputs": [],
   "source": [
    "lowest_resid = float('inf')\n",
    "lowest_mse = float('inf')\n",
    "lowest_smape = float('inf')\n",
    "best_model_order = None\n",
    "\n",
    "for n in range(3):\n",
    "    dp = DeterministicProcess(\n",
    "        index=y.index,\n",
    "        constant=True,\n",
    "        order=n+1,\n",
    "        seasonal=True,\n",
    "    )\n",
    "\n",
    "    X = dp.in_sample()\n",
    "    y = subset_df[target]\n",
    "\n",
    "    seasonality_model = LinearRegression()\n",
    "    seasonality_model.fit(X, y)\n",
    "\n",
    "    y_pred = pd.Series(seasonality_model.predict(X), index=X.index)\n",
    "\n",
    "    fig = px.line(subset_df, y=target, markers=True, \n",
    "                  title=f\"Plot of {target} for county = cfips {cfips} : Seasonality-adjusted Linear Regression Model (Order {n+1})\")\n",
    "    \n",
    "    fig.add_scatter(x=y_pred.index, y=y_pred, line=dict(dash='dash'), name=f\"Order {n+1} Trend\")\n",
    "    \n",
    "    fig.update_layout(showlegend=True)\n",
    "    fig.show()\n",
    "    fig.write_image('my_plotly_chart_4.png')\n",
    "    \n",
    "    # Compute cumulative residual valuation and mean squared error\n",
    "    resid_val = ((y - y_pred)**2).sum()\n",
    "    mse_val = mean_squared_error(y, y_pred)\n",
    "    smape_val = np.mean(np.abs((y - y_pred) / (y + y_pred) / 2)) * 100\n",
    "    \n",
    "    # Check if current model has lower residual valuation or mean squared error\n",
    "    if resid_val < lowest_resid:\n",
    "        lowest_resid = resid_val\n",
    "        best_model_order = n+1\n",
    "        \n",
    "    if mse_val < lowest_mse:\n",
    "        lowest_mse = mse_val\n",
    "        \n",
    "        \n",
    "    if smape_val < lowest_smape:\n",
    "        lowest_smape = smape_val\n",
    "        \n",
    "# Print out results for model with lowest cumulative residual valuation and mean squared error\n",
    "print(f\"The best model is the one with order {best_model_order}, which has a cumulative residual valuation of {lowest_resid:.2f}, a mean squared error of {lowest_mse:.2f}, and a SMAPE of {lowest_smape:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![my_plotly_chart_4.png](attachment:55e57e43-d86d-47e0-826f-0bebc4790f4d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lWd4MjogQ_C"
   },
   "source": [
    "### Decision Tree Regressor Model\n",
    "\n",
    "Having minimized error across MSE and cumulative residual valuations in the seasonality-adjusted linear model above, we now turn to the decision tree model to see how we can impact accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "ktl-1dEYIAJQ",
    "outputId": "742d8e6e-58c2-4021-abbd-c1cefd33041e"
   },
   "outputs": [],
   "source": [
    "X = dp.in_sample()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "mrNlgRjQG4Xj",
    "outputId": "566f2ebe-7e91-4d3d-d676-1b5b6ed919aa"
   },
   "outputs": [],
   "source": [
    "fig = px.line(y, markers=True, \n",
    "              title=f\"Plot of {target} for county = cfips {cfips} De-trended and de-seasoned time series\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image('my_plotly_chart_5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![my_plotly_chart_5.png](attachment:66b5d94e-5113-44f5-bbd1-1a809a1d1853.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df_rfE1KG4US"
   },
   "outputs": [],
   "source": [
    "y = y - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_pacf(y, lags=12)\n",
    "plt.title(f\"PACF Plot of {target}\")\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"Partial Autocorrelation\")\n",
    "plt.savefig(\"pacf_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qvoL_-XwVeE"
   },
   "source": [
    "With this model, we will take care to address partial autocorrelations in our microbusiness_density valuations across each month of the calendar year--based on Choudhary's Kaggle Notebook (2023, February 15th), the code block below represents cycles of temporal autocorrelations across microbusines_density values for a particular cfips (county).\n",
    "\n",
    "For the example county below, it seems months 2, 4, 6, and 9 represent points for which the forecasting model with cfips (6065) needs to reset itself essentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oY5XuzZEHgn2",
    "outputId": "ccf4794b-6e18-4bcb-ec5b-e0e9d63ce4b4"
   },
   "outputs": [],
   "source": [
    "X = X.dropna()\n",
    "y = y[X.index]\n",
    "\n",
    "smape_vals = []\n",
    "for max_depth in range(2, 11):\n",
    "    cycle_model = DecisionTreeRegressor(max_depth=max_depth)  # max_depth is a hyper-parameter that requires careful tuning\n",
    "    cycle_model.fit(X, y)\n",
    "    y_pred = pd.Series(cycle_model.predict(X), index=X.index)\n",
    "\n",
    "    fig = px.line(y, markers=True, title=f\"Plot of {target} for county = cfips {cfips} with Decision Tree Model (max_depth={max_depth})\")\n",
    "    fig.add_scatter(x=y_pred.index, y=y_pred, line=dict(dash='dash'), name=\"Decision Tree Model Forecast\")\n",
    "    fig.update_layout(showlegend=True)\n",
    "    fig.show()\n",
    "    fig.write_image('my_plotly_chart_6.png')\n",
    "\n",
    "    # Compute SMAPE\n",
    "    smape_val = 100/len(y) * np.sum(2 * np.abs(y - y_pred) / (np.abs(y) + np.abs(y_pred)))\n",
    "    smape_vals.append(smape_val)\n",
    "    print(f\"The Decision Tree model with max_depth={max_depth} has a SMAPE of {smape_val:.2f}.\")\n",
    "\n",
    "best_max_depth = np.argmin(smape_vals) + 2  # add 2 to account for range starting at 2\n",
    "print(f\"The best max_depth for the Decision Tree model is {best_max_depth} with a SMAPE of {smape_vals[best_max_depth-2]:.2f}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![my_plotly_chart_6.png](attachment:49ce9fa3-7118-4222-8168-eea9f1a7f3f0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyN9zGsYIOVs"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame()\n",
    "for i in [2, 4, 6, 9]:\n",
    "    X[f'y_lag_{i}'] = y.shift(i)\n",
    "    \n",
    "X = X.dropna()\n",
    "y = y[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "nnFgAmZFIOYt",
    "outputId": "7997d71e-a278-44f2-9f62-542711e80cd5"
   },
   "outputs": [],
   "source": [
    "cycle_model = DecisionTreeRegressor(max_depth=2)  # max_depth is a hyper-parameter that requires careful tuning\n",
    "cycle_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JTYCBTyIOb0"
   },
   "outputs": [],
   "source": [
    "y_pred = pd.Series(cycle_model.predict(X), index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "bubJpcr8IOe3",
    "outputId": "7fd20695-f03d-4fc3-ca4f-07ffe3692382"
   },
   "outputs": [],
   "source": [
    "fig = px.line(y, markers=True, title=f\"Plot of {target} for county = cfips {cfips} : Decision Tree Model with max_depth =2\")\n",
    "fig.add_scatter(x=y_pred.index, y=y_pred, line=dict(dash='dash'))\n",
    "fig.update_layout(showlegend=True)\n",
    "#fig.show()\n",
    "fig.write_image('my_plotly_chart_7.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![my_plotly_chart_7.png](attachment:65cb8291-6c77-47dd-a32e-2441935e07d3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmDGd0sFZfnT"
   },
   "source": [
    "### Decision Tree Reflection\n",
    "\n",
    "The decision tree produces excellent results in terms of its SMAPE accuracy across the board, becoming increasingly biased yet accurate as 'max_depth' hyperparameter increases. \n",
    "\n",
    "\n",
    "To avoid overfitting in this large dataset, we will not increase max_depth beyond 2 in our final combined forecasting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4t_WT3wzD6u"
   },
   "source": [
    "### Combined Model (mixing Linear Regressions and Decision Trees)\n",
    "\n",
    "For the combined model below, we now have a method implemented via Linear Regression and Decision Tree to approximate fairly accurate curves to fit existing trends in microbusiness_density.\n",
    "\n",
    "Knowing we have appropriated our model via decision tree to address a particular county's seasonal cycle of fluctuations in microbusiness_density, we can now build a larger model to address fluctuations across each geography with the time series.\n",
    "\n",
    "This is where we now turn to widening our model's capacities for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_qVHDx-R-4G"
   },
   "outputs": [],
   "source": [
    "def predict_county_microbusinesses(cfips_id, ts_data, horizon_len=1, num_lag_features=1, pacf_threshold=0.0):\n",
    "    dp = DeterministicProcess(\n",
    "        index=ts_data.index,\n",
    "        constant=True,\n",
    "        order=2,\n",
    "        drop=True,\n",
    "        seasonal=True\n",
    "    )\n",
    "    \n",
    "    # Trend and sesonality with Linear Model information\n",
    "    X_trend_train = dp.in_sample()\n",
    "    y_trend_train = ts_data[target]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_trend_train, y_trend_train)\n",
    "\n",
    "    y_pred_trend_train = pd.Series(model.predict(X_trend_train), index=X_trend_train.index)\n",
    "\n",
    "    X_trend_test = dp.out_of_sample(steps=horizon_len)\n",
    "    y_pred_trend_test = pd.Series(model.predict(X_trend_test), index=X_trend_test.index)\n",
    "\n",
    "    # Cycles and autocorrelation accounted via Decision Tree for forecasting ability\n",
    "\n",
    "    y_cycle_train = y_trend_train - y_pred_trend_train  # detrended and deseasoned\n",
    "    \n",
    "    # if pacf threshold provided, select lag features based on correlelogram\n",
    "    if pacf_threshold > 0.0:        \n",
    "        lag_features = np.where(np.abs(pacf(y, 12))>=pacf_threshold)[0][1:]\n",
    "    else:\n",
    "        lag_features = np.arange(1, num_lag_features+1)\n",
    "    X_cycle_train = pd.concat({f\"lag_{i}\":ts_data[target].shift(i) for i in lag_features}, axis=1).dropna()\n",
    "    y_cycle_train = y_cycle_train.loc[X_cycle_train.index]\n",
    "\n",
    "    y_pred_cycle_test = pd.Series()\n",
    "\n",
    "    cycle_model = DecisionTreeRegressor(max_depth=None)\n",
    "    cycle_model.fit(X_cycle_train, y_cycle_train)\n",
    "\n",
    "    y_pred_cycle_train = pd.Series(cycle_model.predict(X_cycle_train), index=X_cycle_train.index)\n",
    "\n",
    "    X_rolling_window = ts_data[target].values#.tolist()\n",
    "\n",
    "    #identify lag resets based on autocorrelation shifts for each unique county\n",
    "    for i in X_trend_test.index:\n",
    "        y_pred_cycle_test[i] = cycle_model.predict([X_rolling_window[[-c for c in lag_features]]])[0]\n",
    "        np.append(X_rolling_window, y_pred_cycle_test[i])\n",
    "\n",
    "    y_pred_train = y_pred_trend_train.loc[y_pred_cycle_train.index] + y_pred_cycle_train\n",
    "\n",
    "    y_pred_test = y_pred_trend_test + y_pred_cycle_test\n",
    "\n",
    "    y_pred_test.name = target\n",
    "    y_pred_test.index.name = 'first_day_of_month'\n",
    "\n",
    "    y_pred_test = y_pred_test.reset_index()\n",
    "    y_pred_test['row_id'] = y_pred_test['first_day_of_month'].dt.date.apply(lambda x: f\"{cfips_id}_{x}\")\n",
    "    \n",
    "    return y_pred_train, y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3QuIlQsQU_Q"
   },
   "outputs": [],
   "source": [
    "def plot_cfips_ts(cfips_id, horizon_len=1, pacf_threshold=0.0, num_lag_features=1):\n",
    "    ts_data = train_df.loc[train_df['cfips']==cfips_id].set_index('first_day_of_month').asfreq('MS')\n",
    "    y_pred_train, y_pred_test = predict_county_microbusinesses(cfips_id, ts_data, horizon_len, num_lag_features, pacf_threshold)\n",
    "    fig = px.line(ts_data[target], markers=True, title=f\"{ts_data['county'][0]}, {ts_data['state'][0]} - cfips {cfips_id}\")\n",
    "    fig.add_scatter(x=y_pred_train.index, y=y_pred_train, line=dict(dash='dashdot'), name='train predicted')\n",
    "    fig.add_scatter(x=y_pred_test['first_day_of_month'], y=y_pred_test[target], line=dict(dash='dash'), name='test predicted')\n",
    "    fig.update_layout(showlegend=True, yaxis_title=target)\n",
    "    fig.show()\n",
    "    fig.write_image('my_plotly_chart_8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiD1PF-DQdDY"
   },
   "outputs": [],
   "source": [
    "cfips_id = np.random.choice(train_df['cfips'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "Gk9kuQ-pQgAR",
    "outputId": "e17c542b-2262-4e2b-897a-5f2b79fcfbf6"
   },
   "outputs": [],
   "source": [
    "plot_cfips_ts(cfips_id, horizon_len=10, pacf_threshold=0.0, num_lag_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![my_plotly_chart_8.png](attachment:d7ba68d2-831e-4ef3-93ad-2df3a9ce861f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jyzyj05uIeTR",
    "outputId": "9ec15b19-0540-4699-e889-ca90644a7b94"
   },
   "outputs": [],
   "source": [
    "submission_df = []\n",
    "\n",
    "for cfips_id in tqdm(test_df['cfips'].unique()):\n",
    "    ts_data = train_df.loc[train_df['cfips']==cfips_id].set_index('first_day_of_month').asfreq('MS')\n",
    "    y_pred_train, y_pred_test = predict_county_microbusinesses(cfips_id, ts_data, horizon_len=8, num_lag_features=2)  # predictions for the next eight months\n",
    "    submission_df.append(y_pred_test)\n",
    "\n",
    "submission_df = pd.concat(submission_df).drop(columns='first_day_of_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "kCiRGgM8IeXB",
    "outputId": "07b9b55f-d3f9-447e-b69f-42152608e84c"
   },
   "outputs": [],
   "source": [
    "train_df['row_number'] = train_df.reset_index().index\n",
    "submission_df['row_number'] = submission_df.reset_index().index\n",
    "merged_df = train_df.merge(submission_df, on='row_number')\n",
    "merged_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SMAPE\n",
    "def smape(actual, forecast):\n",
    "    return 100/len(actual) * np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast)))\n",
    "\n",
    "# Calculate SMAPE and MAE\n",
    "smape_score = smape(merged_df['microbusiness_density_x'], merged_df['microbusiness_density_y'])\n",
    "mae_score = np.mean(np.abs(merged_df['microbusiness_density_x'] - merged_df['microbusiness_density_y']))\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(merged_df['microbusiness_density_x'], merged_df['microbusiness_density_y'])\n",
    "\n",
    "# Calculate AIC\n",
    "n = len(merged_df)\n",
    "k = 2  # two parameters: intercept and slope\n",
    "residuals = merged_df['microbusiness_density_x'] - merged_df['microbusiness_density_y']\n",
    "squared_errors = residuals ** 2\n",
    "rss = squared_errors.sum()\n",
    "aic = n * np.log(rss/n) + 2 * k\n",
    "\n",
    "print(f'SMAPE: {smape_score:.4f}')\n",
    "print(f'MAE: {mae_score:.4f}')\n",
    "print(f'R-squared: {r_squared:.4f}')\n",
    "print(f'AIC: {aic:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SMAPE function\n",
    "def smape(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the Symmetric Mean Absolute Percentage Error between two arrays\n",
    "    \"\"\"\n",
    "    return np.mean((np.abs(actual - predicted) * 200 / (np.abs(actual) + np.abs(predicted))))\n",
    "\n",
    "# Initialize lists to store errors and coefficients\n",
    "smape_x_list = []\n",
    "smape_y_list = []\n",
    "mae_x_list = []\n",
    "mae_y_list = []\n",
    "r_squared_x_list = []\n",
    "r_squared_y_list = []\n",
    "aic_x_list = []\n",
    "aic_y_list = []\n",
    "\n",
    "# Get list of unique cfips values in merged_df\n",
    "cfips_list = merged_df['cfips'].unique()\n",
    "\n",
    "# Loop over each cfips value and calculate errors and coefficients\n",
    "for cfips in cfips_list:\n",
    "    # Subset merged_df for current cfips value\n",
    "    df = merged_df.loc[merged_df['cfips'] == cfips]\n",
    "    \n",
    "    # Get actual and predicted values for 'microbusiness_density_x'\n",
    "    actual_x = df['microbusiness_density_x']\n",
    "    predicted_x = df['microbusiness_density_x']\n",
    "    \n",
    "    # Calculate errors for 'microbusiness_density_x'\n",
    "    smape_x = smape(actual_x, predicted_x)\n",
    "    smape_x_list.append(smape_x)\n",
    "    mae_x = mean_absolute_error(actual_x, predicted_x)\n",
    "    mae_x_list.append(mae_x)\n",
    "    r_squared_x = r2_score(actual_x, predicted_x)\n",
    "    r_squared_x_list.append(r_squared_x)\n",
    "    aic_x = smf.ols(formula=f\"microbusiness_density_x ~ first_day_of_month + active\", data=df).fit().aic\n",
    "    #aic_x = smf.ols(formula=f\"microbusiness_density_x ~ first_day_of_month + active\", data=df).fit().aic\n",
    "    aic_x_list.append(aic_x)\n",
    "    \n",
    "    # Get actual and predicted values for 'microbusiness_density_y'\n",
    "    actual_y = df['microbusiness_density_y']\n",
    "    predicted_y = df['active'] * df['microbusiness_density_y']\n",
    "    \n",
    "    # Calculate errors for 'microbusiness_density_y'\n",
    "    smape_y = smape(actual_y, predicted_y)\n",
    "    smape_y_list.append(smape_y)\n",
    "    mae_y = mean_absolute_error(actual_y, predicted_y)\n",
    "    mae_y_list.append(mae_y)\n",
    "    r_squared_y = r2_score(actual_y, predicted_y)\n",
    "    r_squared_y_list.append(r_squared_y)\n",
    "    aic_y = smf.ols(formula=f\"microbusiness_density_y ~ first_day_of_month + active\", data=df).fit().aic\n",
    "    aic_y_list.append(aic_y)\n",
    "\n",
    "# Calculate average errors and coefficients\n",
    "avg_smape_x = np.mean(smape_x_list)\n",
    "avg_smape_y = np.mean(smape_y_list)\n",
    "avg_mae_x = np.mean(mae_x_list)\n",
    "avg_mae_y = np.mean(mae_y_list)\n",
    "avg_r_squared_x = np.mean(r_squared_x_list)\n",
    "avg_r_squared_y = np.mean(r_squared_y_list)\n",
    "avg_aic_x = np.mean(aic_x_list)\n",
    "avg_aic_y = np.mean(aic_y_list)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average SMAPE for microbusiness_density_x: {avg_smape_x}\")\n",
    "print(f\"Average SMAPE for microbusiness_density_y: {avg_smape_y}\")\n",
    "print(f\"Average MAE for microbusiness_density_x: {avg_mae_x}\")\n",
    "print(f\"Average MAE for microbusiness_density_y: {avg_mae_y}\")\n",
    "print(f\"Average R-squared for microbusiness_density_x: {r_squared_x}\")\n",
    "print(f\"Average R-squared for microbusiness_density_y: {r_squared_y}\")\n",
    "print(f\"Average AIC for aic_x: {aic_x}\")\n",
    "print(f\"Average AIC for aic_y: {aic_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.loc[merged_df['cfips'] == cfips]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SMAPE function\n",
    "def smape(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the Symmetric Mean Absolute Percentage Error between two arrays\n",
    "    \"\"\"\n",
    "    return np.mean((np.abs(actual - predicted) * 200 / (np.abs(actual) + np.abs(predicted))))\n",
    "\n",
    "# Initialize lists to store errors and coefficients\n",
    "smape_x_list = []\n",
    "\n",
    "mae_x_list = []\n",
    "\n",
    "r_squared_x_list = []\n",
    "\n",
    "aic_x_list = []\n",
    "\n",
    "\n",
    "# Get list of unique cfips values in merged_df\n",
    "cfips_list = merged_df['cfips'].unique()\n",
    "\n",
    "# Loop over each cfips value and calculate errors and coefficients\n",
    "for cfips in cfips_list:\n",
    "    # Subset merged_df for current cfips value\n",
    "    df = merged_df.loc[merged_df['cfips'] == cfips]\n",
    "    \n",
    "    # Get actual and predicted values for 'microbusiness_density_x'\n",
    "    actual_x = df['microbusiness_density_x']\n",
    "    predicted_y = df['microbusiness_density_y']\n",
    "    \n",
    "    # Calculate errors for 'microbusiness_density_x'\n",
    "    smape_x = smape(actual_x, predicted_y)\n",
    "    smape_x_list.append(smape_x)\n",
    "    mae_x = mean_absolute_error(actual_x, predicted_y)\n",
    "    mae_x_list.append(mae_x)\n",
    "    r_squared_x = r2_score(actual_x, predicted_y)\n",
    "    r_squared_x_list.append(r_squared_x)\n",
    "    aic_x = smf.ols(formula=f\"microbusiness_density_x ~ first_day_of_month + microbusiness_density_y\", data=df).fit().aic\n",
    "    #aic_x = smf.ols(formula=f\"microbusiness_density_x ~ first_day_of_month + active\", data=df).fit().aic\n",
    "    aic_x_list.append(aic_x)\n",
    "    \n",
    "\n",
    "# Calculate average errors and coefficients\n",
    "avg_smape_x = np.mean(smape_x_list)\n",
    "\n",
    "avg_mae_x = np.mean(mae_x_list)\n",
    "\n",
    "avg_r_squared_x = np.mean(r_squared_x_list)\n",
    "\n",
    "avg_aic_x = np.mean(aic_x_list)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Average SMAPE for microbusiness_density_x: {avg_smape_x}\")\n",
    "\n",
    "print(f\"Average MAE for microbusiness_density_x: {avg_mae_x}\")\n",
    "\n",
    "print(f\"Average R-squared for microbusiness_density_x: {r_squared_x}\")\n",
    "\n",
    "print(f\"Average AIC for aic_x: {aic_x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgIkUSetVX49"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "With the final combined forecasting model, we have predicted values across all 25,080 rows of data as well as forecasting provisions for 8 months beyond the timeframe of the known datapoints beyond January, 2023.\n",
    "\n",
    "Next steps in assessing this model might include post-hoc comparison of actual vs. projected microbusiness density for February-October,2023--it would be very interesting to see exactly how accurate our projections become across counties in terms of their cumulative and month-by-month residual rates, MSE, and SMAPE valuations.\n",
    "\n",
    "Given that certain counties and states inevitably ended up showing better overall accuracy in terms of regression fit and SMAPE valuation, a step that could be taken to improve this model is to split data into logical geographic regions and then merge several decision trees into an updated combined model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-2lD44t94Ur"
   },
   "source": [
    "### References\n",
    "\n",
    "Brownlee, J. (2016, December 2). \"What Is Time Series Forecasting?\" in Machine Learning Mastery. \n",
    "> https://machinelearningmastery.com/time-series-forecasting/\n",
    "\n",
    "Choudhary, C. (2023, Februarey 15th). \"Time series hybrid modeling\" from GoDaddy Microbusiness Forecasting [Codebook]. Kaggle.  \n",
    "> https://kaggle.com/code/ch124uec/time-series-hybrid-modeling\n",
    "\n",
    "GPreda. (2023). \"GoDaddy Data Cleaning and EDA\" from GoDaddy Microbusiness Density Forecasting [Codebook]. Kaggle. \n",
    "> https://www.kaggle.com/code/gpreda/godaddy-data-cleaning-and-eda#Check-the-cfips-from-census-data\n",
    "\n",
    "Holbrook, R. (n.d.). \"Time series: apply machine learning to real-world forecasting tasks\" from Kaggle Learn.\n",
    "> https://wwww.kaggle.com/learn/time-series\n",
    "\n",
    "Kudelya, V. (2023). \"Simple baseline with EDA and SMAPE behaviour\" from GoDaddy Microbusiness Density Forecasting [Codebook]. Kaggle.\n",
    "> https://www.kaggle.com/code/vitalykudelya/simple-baseline-with-eda-and-smape-behaviour\n",
    "\n",
    "RCBhatt. (2023). GoDaddy Microbusiness Density Forecasting [Codebook]. Kaggle. \n",
    "> https://www.kaggle.com/code/rcbhatt/godaddy-microbusiness-density-forecasting"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
